\documentclass[a4paper,12p]{article}
\usepackage{standalone}
\usepackage{polski}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{tikz}
\usepackage{amssymb}
\usepackage{indentfirst}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{graphicx}
\linespread{1.5}

\renewcommand{\refname}{Źródła}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\setcounter{section}{-1}

\begin{document}

\thispagestyle{empty}
	\begin{center}
		{\scshape\Large Wydział Matematyki i Nauk Informacyjnych Politechniki Warszawskiej \par}
		\vspace{3cm}
		{\huge\bfseries WAE - specyfikacja wstępna\par}\
		\vspace{1cm}
		{\Large\scshape Optymalizacja parametrów XGBoost\par}
		\vspace{3cm}
		{\scshape Tomasz Laskowski\par}
		{\scshape Michał Omelańczuk\par}
		\vspace{8cm}
		{\Large \today}
		\vspace{1cm}
	\end{center}
	
	\newpage

\section{Cel projektu}

Cel projektu jest użycie algorytmu optymalizacji do strojenia parametrów metody XGBoost oraz porównanie wyników z modelami z losowym przydziałem parametrów dla wybranych zadań klasyfikacji i regresji.

\section{Model}

\subsection{Wstęp}

\textbf{XGBoost} jest skrótem od \textit{Extreme Gradient Boosting}. Metoda stanowi ulepszenie znanej metody \textbf{Gradient Boosting}. Wykorzystuje uczenie nadzorowane i jest stosowana do problemów regresyjnych i klasyfikacyjnych. Klasyfikatory otrzymywane przez metodę nazywane są zwykle \textbf{boosted trees}, funkcjonując również pod skrótem \textbf{GBM}.

\subsection{Funkcja celu}

Funkcja celu, podobnie jak w wielu innych metodach składa się z dwóch komponentów: funkcji straty $L$ oraz wyrażenia odpowiadającego za regularyzację $\Omega$ (ang. \textit{regularization term}). Jako funkcji straty możemy używać rozmaitych funkcji np. klasycznego błędu średniokwadratowego. $\Omega$ stanowi współczynnik, którego zadaniem jest niedopuszczenie do nadmiernego dopasowania (ang. \textit{overfitting}). Funkcja celu przedstawia się więc następująco:

\begin{center}
	$obj(\Theta)=L(\Theta)+\Omega(\Theta)$
\end{center}

\subsection{Model}

Model składa się ze zbioru \textbf{drzew CART}. Najważniejszą cechą tych drzew, względem zwykłych drzew decyzyjnych jest przechowywanie liczbowego wyniku w liściach, przez co nie dostajemy jedynie prostej klasyfikacji, ale również wartość, którą można zmieniać. W związku z tym formułę dla modelu możemy zdefiniować następująco:

\begin{center}
	$y(x) = \sum_{j=1}{k} f_k{x}, f \in \mathbb{F},$
\end{center}

gdzie $\mathbb{F}$ jest zbiorem funkcji reprezentujących drzewa CART, a $x$ obiektem do klasyfikacji.

\subsection{Trening}

Trening modelu będzie polegał na trenowaniu funkcji $f_i$ reprezentujących kolejne drzewa, wchodzące w skład modelu. Model zwykle budowany jest przyrostowo, tzn. dla każdej iteracji dodajemy kolejne drzewo do modelu. Odpowiada to dodatkowej funkcji $f_i$ funkcji w formule $y$. Jeżeli przez $y^{(i)}$ znaczymy model po $i$-tej iteracji, to dostaniemy uproszczony wzór:

\begin{center}
	$y^{(1)}(x) = f_1(x)$
\end{center}

\begin{center}
	$y^{(t)}(x) = y^{(t-1)}(x) + f_t(x)$
\end{center}

Dla każdego kroku szukamy oczywiście drzewa (funkcji) minimalizującego funkcję straty.

\subsection{Zalety}

XGBoost prezentuje kilka zalet, które są szczególnie widoczne w porównaniu z wyjściową metodą Gradient Boosting:

\begin{enumerate}
	\item \textbf{Regularyzacja}: Standardowa implementacja Gradient Boosting jest bardziej wrażliwa na \textit{overfitting}.
	\item \textbf{Przycinanie drzew}: XGBoost zaczyna przycinanie po osiągnięciu głębokości drzewa, określonego parametrem \texttt{max\_depth}
	\item \textbf{Elastyczność}: Wysokie możliwości parametryzacji i definiowania celu.
	\item \textbf{Obliczenia równoległe}: Mogą być użyte, ale w nieoczywisty sposób \cite{parallel} (drzewa tworzymy iteracyjnie).
\end{enumerate}


\section{Technologia}

Do wykonania pomiarów, związanych z projektem użyjemy biblioteki \texttt{xgboost} dla języka \texttt{Python}.

\section{Parametry}

Implementacja w użytej bibliotece, obok drzew, pozwala również na użycie liniowych klasyfikatorów. Ich skuteczność w znakomitej większości przypadków jest jednak niższa od drzew, dlatego skupimy się wyłącznie na nich. Do wybranych parametrów, których działanie można zbadać należą:

\begin{itemize}
	\item \texttt{max\_depth} - maksymalna głębokość drzewa (jako najdłuższa ścieżka od rdzenia do liścia). Można też zastąpić \texttt{max\_leaf\_nodes} - maksymalną liczbą liści.
	\item \texttt{eta} - \textit{learning rate}, intuicyjnie: opisuje jak szybko klasyfikator porzuca stare własności przy "nauczeniu się" nowych danych.
	\item \texttt{alpha}, \texttt{lambda} - odpowiadają za różne typy regularyzacji, odpowiednio \textbf{L1} i \textbf{L2}. 
	\item \texttt{gamma} - definiuje minimalną wartość zysku przy którym dochodzi do podziału węzła, zapewnia nam to że nie będzie za dużo podziału węzłów.
\end{itemize}

Wszystkie dostępne parametry są opisane w dokumentacji biblioteki \cite{github}.

\section{Zbiory danych}
Zbiór danych który będziemy wykorzystywać to \url{https://www.kaggle.com/c/forest-cover-type-prediction/data} zawarte są w nim dane i na ich podstawie będziemy oceniać jaki typ lasu rośnie na danym terenie.


% do uzupełnienia

\begin{thebibliography}{1}
\bibitem{docs} Dokumentacja XGBoost \url{http://xgboost.readthedocs.io/en/latest/model.html}
\bibitem{tutorial} Tutorial Analytics Vidhya o XGBoost \url{https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/}
\bibitem{parallel} Parallel Gradient Boosting Decision Trees \url{http://zhanpengfang.github.io/418home.html}
\bibitem{github} Biblioteka XGBoost \url{https://github.com/dmlc/xgboost}
\end{thebibliography}

\end{document}
